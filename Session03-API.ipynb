{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 3: Harvesting data from the web: APIs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A first API\n",
    "\n",
    "[Chronicling America](http://chroniclingamerica.loc.gov/about/) is a joint project of the National Endowment for the Humanities and the Library of Congress .\n",
    "\n",
    "Search for articles that mention \"[slavery](http://chroniclingamerica.loc.gov/search/pages/results/?andtext=slavery)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Look at the URL. What happens if you change the word slavery to abolition? \n",
    "\n",
    "What happens to the URL when you go to the second page? Can you get to page 251?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we append ``&format=json`` to the end of the search URL? \n",
    "\n",
    "\n",
    "http://chroniclingamerica.loc.gov/search/pages/results/?andtext=slavery&format=json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[``requests``](http://docs.python-requests.org/en/master/) is a useful and commonly used HTTP library for python. It is not a part of the default installation, but is included with Anaconda Python Distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It would be possible to use the API URL and parameters directly in the requests command, but since the most likely scenario involves making repeating calls to ``requests`` as part of a loop -- the search returned less than 1% of the results -- I store the strings first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "base_url =  'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "parameters = '?andtext=slavery&format=json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`requests.get()` is used for both accessing websites and APIs. The command can be modified by several arguements, but at a minimum, it requires the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r = requests.get(base_url + parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`r` is a `requests` response object. Any JSON returned by the server are stored in `.json().`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "search_json = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "JSONs are dictionary like objects, in that they have keys (think variable names) and values. `.keys()` returns a list of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'totalItems', u'endIndex', u'startIndex', u'itemsPerPage', u'items']\n"
     ]
    }
   ],
   "source": [
    "print search_json.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can return the value of any key by putting the key name in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432779"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_json['totalItems']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As is often the case with results from an API, most of the keys and values are metadate about either the search or what is being returned. These are useful for knowing if the search is returning what you want, which is particularly important when you are making multiple calls to the API. \n",
    "\n",
    "The data I'm intereted in is all in `items`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print type(search_json['items'])\n",
    "print len(search_json['items'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`items` is a list with 20 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n",
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print type(search_json['items'][0])\n",
    "print type(search_json['items'][19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each of the 20 items in the list is a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'sequence', u'county', u'edition', u'frequency', u'id', u'section_label', u'city', u'date', u'title', u'end_year', u'note', u'state', u'subject', u'type', u'place_of_publication', u'start_year', u'edition_label', u'publisher', u'language', u'alt_title', u'lccn', u'country', u'ocr_eng', u'batch', u'title_normal', u'url', u'place', u'page']\n"
     ]
    }
   ],
   "source": [
    "first_item = search_json['items'][0]\n",
    "\n",
    "print first_item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "What is the title of the first item?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "While a standard CSV file has a header row that describes the contents of each column, a JSON file has keys identifying the values found in each case. Importantly, these keys need not be the same for each item. Additionally, values don't have to be numbers of strings, but could be lists or dictionaries. For example, this JSON could have included a `newspaper` key that was a dictionary with all the metadata about the newspaper the article and issue was published, an `article` key that include the article specific information as another dictionary, and a `text` key whose value was a string with the article text.\n",
    "\n",
    "As before, we can examine the contents of a particular item, such as the publication's `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti-slavery bugle. volume\n"
     ]
    }
   ],
   "source": [
    "print first_item['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The easiest way to view or analyze this data is to convert it to a dataset-like structure. While Python does not have a builting in dataframe type, the popular `pandas` library does. By convention, it is imported as `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make sure all columns are displayed\n",
    "pd.set_option(\"display.max_columns\",101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "pandas is prety smart about importing different JSON-type objects and converting them to dataframes with its `.DataFrame()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt_title</th>\n",
       "      <th>batch</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>county</th>\n",
       "      <th>date</th>\n",
       "      <th>edition</th>\n",
       "      <th>edition_label</th>\n",
       "      <th>end_year</th>\n",
       "      <th>frequency</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>lccn</th>\n",
       "      <th>note</th>\n",
       "      <th>ocr_eng</th>\n",
       "      <th>page</th>\n",
       "      <th>place</th>\n",
       "      <th>place_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>section_label</th>\n",
       "      <th>sequence</th>\n",
       "      <th>start_year</th>\n",
       "      <th>state</th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>title_normal</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_ohi_ariel_ver02</td>\n",
       "      <td>[New Lisbon, Salem]</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>[Columbiana, Columbiana]</td>\n",
       "      <td>18490316</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1861</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>/lccn/sn83035487/1849-03-16/ed-1/seq-1/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83035487</td>\n",
       "      <td>[Archived issues are available in digital form...</td>\n",
       "      <td>LAVE\\nam\\nJlile\\nVOL. 4. NO. 30.\\nSALEM. OHIO,...</td>\n",
       "      <td></td>\n",
       "      <td>[Ohio--Columbiana--New Lisbon, Ohio--Columbian...</td>\n",
       "      <td>New-Lisbon, Ohio</td>\n",
       "      <td>Ohio American Antislavery Society</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1845</td>\n",
       "      <td>[Ohio, Ohio]</td>\n",
       "      <td>[Antislavery movements--United States--Newspap...</td>\n",
       "      <td>Anti-slavery bugle. volume</td>\n",
       "      <td>anti-slavery bugle.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_golf_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19140516</td>\n",
       "      <td>None</td>\n",
       "      <td>NOON EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1914-05-16/ed-1/seq-10/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>r\\nmmmmmmmmmmmmmmmmmmmmmmmm\\n'SLAVERY RIFE IN ...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_india_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19161109</td>\n",
       "      <td>None</td>\n",
       "      <td>EXTRA</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1916-11-09/ed-1/seq-26/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>us remaining whites if we expect to\\nstay on t...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>26</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_golf_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19150327</td>\n",
       "      <td>None</td>\n",
       "      <td>NOON EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1915-03-27/ed-1/seq-24/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>THOUSANDS OF VEILED WOMEN OF TURKISH\\nHAREM ON...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>24</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_foxtrot_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19130815</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1913-08-15/ed-1/seq-5/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>LOLA NORRiajQlVS SiENSAT-iPN AL t EVIDENCE IN ...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_foxtrot_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19130308</td>\n",
       "      <td>None</td>\n",
       "      <td>NOON EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1913-03-08/ed-1/seq-6/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>that every possible weakness in. a\\ngirl as &amp;e...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  alt_title                     batch                 city   country  \\\n",
       "0        []     batch_ohi_ariel_ver02  [New Lisbon, Salem]      Ohio   \n",
       "1        []     batch_iune_golf_ver01            [Chicago]  Illinois   \n",
       "2        []    batch_iune_india_ver01            [Chicago]  Illinois   \n",
       "3        []     batch_iune_golf_ver01            [Chicago]  Illinois   \n",
       "4        []  batch_iune_foxtrot_ver01            [Chicago]  Illinois   \n",
       "5        []  batch_iune_foxtrot_ver01            [Chicago]  Illinois   \n",
       "\n",
       "                     county      date edition edition_label  end_year  \\\n",
       "0  [Columbiana, Columbiana]  18490316    None                    1861   \n",
       "1             [Cook County]  19140516    None  NOON EDITION      1917   \n",
       "2             [Cook County]  19161109    None         EXTRA      1917   \n",
       "3             [Cook County]  19150327    None  NOON EDITION      1917   \n",
       "4             [Cook County]  19130815    None                    1917   \n",
       "5             [Cook County]  19130308    None  NOON EDITION      1917   \n",
       "\n",
       "                            frequency  \\\n",
       "0                              Weekly   \n",
       "1  Daily (except Sunday and holidays)   \n",
       "2  Daily (except Sunday and holidays)   \n",
       "3  Daily (except Sunday and holidays)   \n",
       "4  Daily (except Sunday and holidays)   \n",
       "5  Daily (except Sunday and holidays)   \n",
       "\n",
       "                                         id   language        lccn  \\\n",
       "0   /lccn/sn83035487/1849-03-16/ed-1/seq-1/  [English]  sn83035487   \n",
       "1  /lccn/sn83045487/1914-05-16/ed-1/seq-10/  [English]  sn83045487   \n",
       "2  /lccn/sn83045487/1916-11-09/ed-1/seq-26/  [English]  sn83045487   \n",
       "3  /lccn/sn83045487/1915-03-27/ed-1/seq-24/  [English]  sn83045487   \n",
       "4   /lccn/sn83045487/1913-08-15/ed-1/seq-5/  [English]  sn83045487   \n",
       "5   /lccn/sn83045487/1913-03-08/ed-1/seq-6/  [English]  sn83045487   \n",
       "\n",
       "                                                note  \\\n",
       "0  [Archived issues are available in digital form...   \n",
       "1  [\"An adless daily newspaper.\", Archived issues...   \n",
       "2  [\"An adless daily newspaper.\", Archived issues...   \n",
       "3  [\"An adless daily newspaper.\", Archived issues...   \n",
       "4  [\"An adless daily newspaper.\", Archived issues...   \n",
       "5  [\"An adless daily newspaper.\", Archived issues...   \n",
       "\n",
       "                                             ocr_eng page  \\\n",
       "0  LAVE\\nam\\nJlile\\nVOL. 4. NO. 30.\\nSALEM. OHIO,...        \n",
       "1  r\\nmmmmmmmmmmmmmmmmmmmmmmmm\\n'SLAVERY RIFE IN ...        \n",
       "2  us remaining whites if we expect to\\nstay on t...        \n",
       "3  THOUSANDS OF VEILED WOMEN OF TURKISH\\nHAREM ON...        \n",
       "4  LOLA NORRiajQlVS SiENSAT-iPN AL t EVIDENCE IN ...        \n",
       "5  that every possible weakness in. a\\ngirl as &e...        \n",
       "\n",
       "                                               place place_of_publication  \\\n",
       "0  [Ohio--Columbiana--New Lisbon, Ohio--Columbian...     New-Lisbon, Ohio   \n",
       "1                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "2                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "3                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "4                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "5                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "\n",
       "                           publisher section_label  sequence  start_year  \\\n",
       "0  Ohio American Antislavery Society                       1        1845   \n",
       "1                       N.D. Cochran                      10        1911   \n",
       "2                       N.D. Cochran                      26        1911   \n",
       "3                       N.D. Cochran                      24        1911   \n",
       "4                       N.D. Cochran                       5        1911   \n",
       "5                       N.D. Cochran                       6        1911   \n",
       "\n",
       "          state                                            subject  \\\n",
       "0  [Ohio, Ohio]  [Antislavery movements--United States--Newspap...   \n",
       "1    [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "2    [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "3    [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "4    [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "5    [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "\n",
       "                        title         title_normal  type  \\\n",
       "0  Anti-slavery bugle. volume  anti-slavery bugle.  page   \n",
       "1               The day book.            day book.  page   \n",
       "2               The day book.            day book.  page   \n",
       "3               The day book.            day book.  page   \n",
       "4               The day book.            day book.  page   \n",
       "5               The day book.            day book.  page   \n",
       "\n",
       "                                                 url  \n",
       "0  http://chroniclingamerica.loc.gov/lccn/sn83035...  \n",
       "1  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "2  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "3  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "4  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "5  http://chroniclingamerica.loc.gov/lccn/sn83045...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(search_json['items'])\n",
    "\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that I converted `search_json['items']` to  dataframe and not the entire JSON file. This is because I wanted each row to be an article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endIndex</th>\n",
       "      <th>items</th>\n",
       "      <th>itemsPerPage</th>\n",
       "      <th>startIndex</th>\n",
       "      <th>totalItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Columbiana', u'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 10, u'county': [u'Cook County'],...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 26, u'county': [u'Cook County'],...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 24, u'county': [u'Cook County'],...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 5, u'county': [u'Cook County'], ...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 6, u'county': [u'Cook County'], ...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 13, u'county': [u'Cook County'],...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [None], u'edition'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 30, u'county': [u'Cook County'],...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 4, u'county': [None], u'edition'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Washington'], u...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 2, u'county': [None], u'edition'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Ottawa'], u'edi...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Ashtabula'], u'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Rutland', u'Was...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 3, u'county': [u'Wood'], u'editi...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 4, u'county': [u'New York', u'Cu...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 10, u'county': [u'New York', u'C...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Washington'], u...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 2, u'county': [None], u'edition'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>432779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    endIndex                                              items  itemsPerPage  \\\n",
       "0         20  {u'sequence': 1, u'county': [u'Columbiana', u'...            20   \n",
       "1         20  {u'sequence': 10, u'county': [u'Cook County'],...            20   \n",
       "2         20  {u'sequence': 26, u'county': [u'Cook County'],...            20   \n",
       "3         20  {u'sequence': 24, u'county': [u'Cook County'],...            20   \n",
       "4         20  {u'sequence': 5, u'county': [u'Cook County'], ...            20   \n",
       "5         20  {u'sequence': 6, u'county': [u'Cook County'], ...            20   \n",
       "6         20  {u'sequence': 13, u'county': [u'Cook County'],...            20   \n",
       "7         20  {u'sequence': 1, u'county': [None], u'edition'...            20   \n",
       "8         20  {u'sequence': 30, u'county': [u'Cook County'],...            20   \n",
       "9         20  {u'sequence': 4, u'county': [None], u'edition'...            20   \n",
       "10        20  {u'sequence': 1, u'county': [u'Washington'], u...            20   \n",
       "11        20  {u'sequence': 2, u'county': [None], u'edition'...            20   \n",
       "12        20  {u'sequence': 1, u'county': [u'Ottawa'], u'edi...            20   \n",
       "13        20  {u'sequence': 1, u'county': [u'Ashtabula'], u'...            20   \n",
       "14        20  {u'sequence': 1, u'county': [u'Rutland', u'Was...            20   \n",
       "15        20  {u'sequence': 3, u'county': [u'Wood'], u'editi...            20   \n",
       "16        20  {u'sequence': 4, u'county': [u'New York', u'Cu...            20   \n",
       "17        20  {u'sequence': 10, u'county': [u'New York', u'C...            20   \n",
       "18        20  {u'sequence': 1, u'county': [u'Washington'], u...            20   \n",
       "19        20  {u'sequence': 2, u'county': [None], u'edition'...            20   \n",
       "\n",
       "    startIndex  totalItems  \n",
       "0            1      432779  \n",
       "1            1      432779  \n",
       "2            1      432779  \n",
       "3            1      432779  \n",
       "4            1      432779  \n",
       "5            1      432779  \n",
       "6            1      432779  \n",
       "7            1      432779  \n",
       "8            1      432779  \n",
       "9            1      432779  \n",
       "10           1      432779  \n",
       "11           1      432779  \n",
       "12           1      432779  \n",
       "13           1      432779  \n",
       "14           1      432779  \n",
       "15           1      432779  \n",
       "16           1      432779  \n",
       "17           1      432779  \n",
       "18           1      432779  \n",
       "19           1      432779  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(search_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If this dataframe contained all the items that you were looking for, it would be easy to save this to a csv file for storage and later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('lynching_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",alt_title,batch,city,country,county,date,edition,edition_label,end_year,frequency,id,language,lccn,note,ocr_eng,page,place,place_of_publication,publisher,section_label,sequence,start_year,state,subject,title,title_normal,type,url\r\n",
      "0,[],batch_ohi_ariel_ver02,\"[u'New Lisbon', u'Salem']\",Ohio,\"[u'Columbiana', u'Columbiana']\",18490316,,,1861,Weekly,/lccn/sn83035487/1849-03-16/ed-1/seq-1/,[u'English'],sn83035487,\"[u'Archived issues are available in digital format as part of the Library of Congress Chronicling America online collection.', u'Editors: Benjamin S. Jones, J. Elizabeth Hitchcock, 1845-1846; Benjamin S. Jones, J. Elizabeth Jones, 1846-1849; Oliver Johnson 1849-1851; Marius R. Robinson, 1851-1859; Benjamin S. Jones, 1859-1861.', u'Not published June 27-July 18, 1845.', u'Printers: John Frost, 1845; J.H. Painter, 1845-1846; G.N. Hapgood, 1846-1848.', u'Published in: New Lisbon, Ohio, June 20, 1845-Aug. 29, 1845, and: Salem, Ohio, Sept. 5, 1845-May 4, 1861.', u'Publisher: Executive Committee of the Western Anti-slavery Society, 1848-1861.']\",\"LAVE\r\n",
      "am\r\n",
      "Jlile\r\n",
      "VOL. 4. NO. 30.\r\n",
      "SALEM. OHIO, FRIDAY, MARCH 1G, 1849.\r\n",
      "WHOLE NO. 186.\r\n",
      "ANTI\r\n",
      "Ti v Tr-\r\n",
      "HI\r\n"
     ]
    }
   ],
   "source": [
    "!head lynching_articles.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is only a small subset of the articles on lynching that are available, however. The API returns results in batches of 20 and this is only the first page of results. As is often the case, I'll need to make multiple calls to the API to retrieve all the data of interest. The easiest way to do that is to define a small function for getting the article information and put that in a loop. While it isn't a requirement that you create a function for making the API call, it will make your code easier to read and debug.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Looking at the API guidelines, there is an additional paramater `page` that tells the API which subset of results we want. This name varies by API but their is usually some mechanism for retrieiving results beyond the initial JSON.\n",
    "\n",
    "Before creating the loop and making multiple calls to the API, I want to make sure that the API is working the way I think it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Look at the API guidelines. How can we get the third page?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "base_url   = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "parameters = '?andtext=slavery&format=json&page=3'\n",
    "\n",
    "r = requests.get(base_url + parameters)\n",
    "results =  r.json()\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A call to random selected page 3 returns results 41 through 60, which is what I expected since each page has 20 items.\n",
    "\n",
    "The parameters are getting pretty ugly, so fortunately `requests` accepts a dictionary where the keys are the parameter names as defined by the API and the values are the search paramaters you are looking for. So the same request can be rewritten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "parameters = {'andtext': 'lynching',\n",
    "              'page' : 3,\n",
    "              'format'  : 'json'}\n",
    "r = requests.get(base_url, params=parameters)\n",
    "\n",
    "results =  r.json()\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This can be rewritten as function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_articles():\n",
    "    '''\n",
    "    Make calls to the Chronicling America API.\n",
    "    '''\n",
    "    \n",
    "    base_url   = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "    parameters = {'andtext': 'lynching',\n",
    "                  'page'   : 3,\n",
    "                  'format' : 'json'}\n",
    "    \n",
    "    r = requests.get(base_url, params = parameters)\n",
    "    results =  r.json()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "results = get_articles()\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The advantage of writing a function, however, would be that you can pass along your own parameters, such as the search term and page number, which would make this much more useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_articles(search_term, page_number):\n",
    "    '''\n",
    "    Make calls to the Chronicling America API.\n",
    "    '''\n",
    "    \n",
    "    base_url = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "    parameters = {'andtext': search_term,\n",
    "                  'page'   : page_number,\n",
    "                  'format' : 'json'}\n",
    "    \n",
    "    r = requests.get(base_url, params = parameters)\n",
    "    results =  r.json()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "results = get_articles('lynching', 3)\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, the first 60 results could downloaded in a just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 20\n",
      "21 40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f91eb7d7ca04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage_number\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# range stops before it gets to the last number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lynching'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'startIndex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'endIndex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-a0c2e842808e>\u001b[0m in \u001b[0;36mget_articles\u001b[0;34m(search_term, page_number)\u001b[0m\n\u001b[1;32m      8\u001b[0m               'format'  : 'json'}\n\u001b[1;32m      9\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m                     return complexjson.loads(\n\u001b[0;32m--> 877\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m                     )\n\u001b[1;32m    879\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/simplejson/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, **kw)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mparse_constant\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             and not use_decimal and not kw):\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/simplejson/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_PY3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/simplejson/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mord0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xef\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\xef\\xbb\\xbf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for page_number in range(1,4): # range stops before it gets to the last number\n",
    "    results = get_articles('lynching', page_number)\n",
    "    print results['startIndex'], results['endIndex']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Everything appears to be working, but unfortunately I only have the last page of results still. Each call to the API was redefining `results` variable. In this case, I set up an empty dataframe to store the results and will append the items from each page of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for page_number in range(1,4):\n",
    "    results = get_articles('lynching', page_number)\n",
    "    new_df = pd.DataFrame(results['items'])\n",
    "    df = df.append(new_df , ignore_index=True)\n",
    "    \n",
    "print len(df)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a large download, you would still want to tweak this a bit by pausing between each API call and making it robust to internet or API errors, but this is a solid framework for collecting data from an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Geocoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Work in groups of three!\n",
    "\n",
    "You have been handed a list of addresses. You want to geocode them. \n",
    "\n",
    "\n",
    "1. Read about the Google Maps Geocoding API.\n",
    "2. On paper, map out your work flow. What functions will you need?\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "My workflow:   \n",
    "   \n",
    "   \n",
    "1. Use `requests` to test out a single address.    \n",
    "2. Turn that into a function that accepts a location.    \n",
    "3. Read in the CSV file with all the locations.    \n",
    "4. Store the results    \n",
    "5. Write them to a CSV.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A second API\n",
    "\n",
    "While the Chronicling America API allows annonymous usage, most APIs require you to register in advance. This usually involves going to their website, signing up for the service, and then going through a second signup for developers.  \n",
    "When you sign up  to use an API, you usually agree to only use the API to facilitate other people using the service (e.g. customer's finding their way to your store) and that you won't store the data. API providers usually enforce this through rate limiting, meaning you can only access the service so many times per minute or per day. For example, you can only search status updates 180 times every 15 minutes according to [Twitter guidelines](https://dev.twitter.com/docs/rate-limiting/1.1/limits). [Yelp](http://www.yelp.com/developers/documentation/faq) limits you to 10,000 calls per day. If you go over your limit, you won't be able to access the service for a bit. You will also get in trouble if you redistribute the data, so don't plan on doing that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the major reasons that web services require API authentication is so that they know who you are and so they can make sure that you don't go over their rate limits. Since you shouldn't be giving your password to random people on the internet, API authentication works a little bit differently. Like many other places, in order to use the Yelp API you have to sign up as [developer](http://www.yelp.com/developers). After telling them a little bit about what you plan to do--feel free to be honest; they aren't going to deny you access if you put \"research on food cultures\" as the purpose--you will get a Consumer Key, Consumer Secret, Token, and Token Secret. Copy and paste them somewhere special. \n",
    "\n",
    "Using the Yelp API goes something like this. First, you tell Yelp who you are and what you want. Assuming you are authorized to have this information, they respond with a URL where you can retrieve the data. The coding for this in practice is a little bit complicated, so there are often single use tools for accessing APIs, like [Tweepy](http://tweepy.github.io) for Twitter. \n",
    "\n",
    "Yelp uses the OAuth protocol for authentication. There are several python libraries for handling this, but you will likely need to install one (via `conda` or `pip`) yourself first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import oauth2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There's no module to install for the Yelp API, but Yelp does provide some [sample Python code](https://github.com/Yelp/yelp-api/tree/master/v2/python). I've slightly modified the code below to show a sample search for restaurants near Chapel Hill, NC, sorted by distance. You can find more options in the search [documentation](http://www.yelp.com/developers/documentation/v2/search_api). The API's search options include things like location and type of business, and allows you to sort either by distance or popularity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key    = 'qDBPo9c_szHVrZwxzo-zDw'\n",
    "consumer_secret = '4we8Jz9rq5J3j15Z5yCUqmgDJjM'\n",
    "token           = 'jeRrhRey_k-emvC_VFLGrlVHrkR4P3UF'\n",
    "token_secret    = 'n-7xHNCxxedmAMYZPQtnh1hd7lI'\n",
    "\n",
    "consumer = oauth2.Consumer(consumer_key, consumer_secret)\n",
    "\n",
    "category_filter = 'restaurants'\n",
    "location = 'Oslo, Norway'\n",
    "options =  'category_filter=%s&location=%s&sort=1' % (category_filter, location)\n",
    "url = 'http://api.yelp.com/v2/search?' + options\n",
    "\n",
    "oauth_request = oauth2.Request('GET', url, {})\n",
    "oauth_request.update({'oauth_nonce'      : oauth2.generate_nonce(),\n",
    "                      'oauth_timestamp'  : oauth2.generate_timestamp(),\n",
    "                      'oauth_token'       : token,\n",
    "                      'oauth_consumer_key': consumer_key})\n",
    "\n",
    "token = oauth2.Token(token, token_secret)\n",
    "oauth_request.sign_request(oauth2.SignatureMethod_HMAC_SHA1(), consumer, token)\n",
    "signed_url = oauth_request.to_url()\n",
    "\n",
    "print signed_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URL returned expires after a couple of seconds, so don't expect for the above link to work. The results are provided in the JSON file format, so I'm going to use the already imported `requests` module to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resp = requests.get(url=signed_url)\n",
    "chapel_hill_restaurants = resp.json()\n",
    "\n",
    "print chapel_hill_restaurants.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Chronacling America API, the top level of the JSON contains some metadata about the search with all the specific items in one field. In this case, `businesses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chapel_hill_restaurants['businesses'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the returned results for one restaraunt, it is clear that Yelp is keeping a lot of the review data for themselves. They returned the overall restaurant `rating`, but they provide only a small bit of text (`snippet_text`) instead of the full reviews and ratings. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print chapel_hill_restaurants['total']\n",
    "print len(chapel_hill_restaurants['businesses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, they cap the total number of business the search will return at 40 and only provide 20 results for each API call.\n",
    "\n",
    "Even with these restrictions, it still might be useful for social science research. As before, you would likely want to define a function in order to make repeated calls to the API. In this, the easier solution might be to create two functions. One that gets a single page and another which retrieves both pages for a single geographical area by calling the first function twice. While it would be possible to do this with zero or one new functions, creating two functions allows for better control over finding and debugging errors since you can test each function independently. Creating lots of small functions generally the code more readable, especially in case like this where you are looping over pages within restaurants within geographic areas. In general, I think the principle of a workflow consisting of small functions, as is commonly found in Python code, is something that social scientists should adopt even when they aren't writing Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_yelp_page(location, offset):\n",
    "    '''\n",
    "    Retrieve one page of results from the Yelp API\n",
    "    Returns a JSON file\n",
    "    '''\n",
    "    # from https://github.com/Yelp/yelp-api/tree/master/v2/python\n",
    "    consumer_key    = 'qDBPo9c_szHVrZwxzo-zDw'\n",
    "    consumer_secret = '4we8Jz9rq5J3j15Z5yCUqmgDJjM'\n",
    "    token           = 'jeRrhRey_k-emvC_VFLGrlVHrkR4P3UF'\n",
    "    token_secret    = 'n-7xHNCxxedmAMYZPQtnh1hd7lI'\n",
    "    \n",
    "    consumer = oauth2.Consumer(consumer_key, consumer_secret)\n",
    "    \n",
    "    url = 'http://api.yelp.com/v2/search?category_filter=restaurants&location=%s&sort=1&offset=%s' % (location, offset)\n",
    "    \n",
    "    oauth_request = oauth2.Request('GET', url, {})\n",
    "    oauth_request.update({'oauth_nonce': oauth2.generate_nonce(),\n",
    "                          'oauth_timestamp': oauth2.generate_timestamp(),\n",
    "                          'oauth_token': token,\n",
    "                          'oauth_consumer_key': consumer_key})\n",
    "    \n",
    "    token = oauth2.Token(token, token_secret)\n",
    "    \n",
    "    oauth_request.sign_request(oauth2.SignatureMethod_HMAC_SHA1(), consumer, token)\n",
    "    \n",
    "    signed_url = oauth_request.to_url()\n",
    "    resp = requests.get(url=signed_url)\n",
    "    return resp.json()\n",
    "\n",
    "def get_yelp_results(location):\n",
    "    '''\n",
    "    Retrive both pages of results from the Yelp API\n",
    "    Returns a dataframe\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    for offset in [1,21]:\n",
    "        results = get_yelp_page(location, offset)\n",
    "        new_df = pd.DataFrame(results['businesses'])\n",
    "        df = df.append(new_df , ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_df = get_yelp_results('Chapel Hill, NC')\n",
    "\n",
    "print len(ch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_df[['name','categories','review_count','rating']].sort_values(by='rating', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function expects that the first thing you input will be a location. Taking advantage of both `oath2`'s ability to clean up the text so that it is functional when put in a URL (e.g., escape spaces) and Yelp's savvy ability to parse locations, the value for location can be fairly wide (e.g., \"Chapel Hill\" or \"90210\"). You can also add a category of business to search for from the [list](http://www.yelp.com/developers/documentation/category_list) of acceptable values. If you don't provide a value, `category_filter = 'restaurants'` provides a default value of 'restaurants'. This function returns the JSON formatted results. Note that this doesn't have any mechanism for handling errors, which will need to happen elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chapel_hill_restaurants = get_yelp_businesses('Chapel Hill, NC', 21)\n",
    "for business in chapel_hill_restaurants['businesses']:\n",
    "    print '%s - %s (%s)' % (business['rating'], business['name'], business['review_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beverly_hills_restaurants = get_yelp_businesses('90210')\n",
    "for business in beverly_hills_restaurants['businesses']:\n",
    "    print '%s - %s (%s)' % (business['rating'], business['name'], business['review_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
